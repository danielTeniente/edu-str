{
    "app": {
        "title": "Machine Learning Libre",
        "welcome": "¬°Bienvenido! üéì",
        "description": "Esta aplicaci√≥n proporciona visualizaciones interactivas para comprender conceptos de machine learning. Utiliza el men√∫ lateral para navegar por las secciones.",
        "available_sections": "Secciones disponibles:",
        "navigation": "Navegaci√≥n",
        "language": "Idioma"
    },
    "perceptron": {
        "title": "El perceptr√≥n",
        "description": "El perceptr√≥n es un concepto clave, pues las redes neuronales modernas utilizan este elemento como bloque de construcci√≥n. \n\n Se trata de una neurona capaz de hacer una clasificaci√≥n binaria en base a una regresi√≥n lineal y una funci√≥n de activaci√≥n.",
        "weight1": "Peso w1",
        "weight2": "Peso w2",
        "bias": "Bias (b)",
        "class1": "Clase 1 (y=1)",
        "class_neg1": "Clase -1 (y=-1)",
        "decision_boundary": "Frontera de decisi√≥n",
        "graph_title": "Clasificaci√≥n del Perceptr√≥n y Frontera de Decisi√≥n",
        "equations_title": "Ecuaciones Matem√°ticas",
        "perceptron_eq_title": "Regla de Decisi√≥n del Perceptr√≥n:",
        "perceptron_eq_desc": "El perceptr√≥n clasifica un punto (x‚ÇÅ, x‚ÇÇ) bas√°ndose en esta ecuaci√≥n:",
        "comparison_title": "Relaci√≥n con el Perceptr√≥n:",
        "comparison_text": "¬°La frontera de decisi√≥n de nuestro perceptr√≥n es una recta! Comparemos ambas ecuaciones:\n\n‚Ä¢ Forma est√°ndar: x‚ÇÇ = mx‚ÇÅ + b\n‚Ä¢ Nuestra frontera: x‚ÇÇ = -($w_1$/$w_2$)x‚ÇÅ - (b/$w_2$)\n\nPor lo tanto:\n‚Ä¢ La pendiente (m) es -w‚ÇÅ/w‚ÇÇ\n‚Ä¢ La ordenada al origen (b) es -b/w‚ÇÇ",
        "boundary_eq_title": "Ecuaci√≥n de la Frontera de Decisi√≥n:",
        "boundary_eq_desc": "La frontera de decisi√≥n es donde la salida del perceptr√≥n es igual a cero. Despejando x‚ÇÇ:",
        "explanation_title": "Entendiendo los Par√°metros:",
        "explanation_text": "‚Ä¢ w‚ÇÅ, w‚ÇÇ: Pesos que determinan la pendiente de la frontera de decisi√≥n\n\n‚Ä¢ b: T√©rmino de bias, determina d√≥nde la l√≠nea intersecta el eje x‚ÇÇ (desplazamiento vertical)\n‚Ä¢ La frontera de decisi√≥n es la l√≠nea que separa las dos clases\n‚Ä¢ Los puntos sobre la l√≠nea se clasifican como una clase, los puntos debajo como la otra\n‚Ä¢ Cuando w‚ÇÇ = 0, la frontera se convierte en una l√≠nea vertical en x‚ÇÅ = -b/w‚ÇÅ",
        "exercise_title": "Ejercicio Pr√°ctico",
        "exercise_desc": "Intenta clasificar estos puntos ajustando los pesos y el bias. El objetivo es encontrar una l√≠nea que separe los puntos azules de los rojos.",
        "exercise_plot_title": "Clasificaci√≥n Pr√°ctica",
        "exercise_success": "¬°Felicitaciones! Has clasificado correctamente todos los puntos.",
        "exercise_continue": "¬°Sigue intentando! Ajusta los pesos y el bias para encontrar una l√≠nea que separe las clases.",
        "limitations_title": "Limitaciones del Perceptr√≥n",
        "limitations_desc": "El perceptr√≥n solo puede aprender patrones linealmente separables. Aqu√≠ hay un ejemplo de un conjunto de datos no linealmente separable que un perceptr√≥n no puede clasificar correctamente:",
        "limitations_plot_title": "Datos No Linealmente Separables",
        "limitations_text": "En este ejemplo, tenemos dos clases de puntos que forman un patr√≥n circular. Ninguna l√≠nea recta (la frontera de decisi√≥n del perceptr√≥n) puede separar estos puntos correctamente. Esta es una de las principales limitaciones del modelo perceptr√≥n.",
        "division_by_zero": "Cuando w‚ÇÇ = 0, la frontera de decisi√≥n se convierte en una l√≠nea vertical en x‚ÇÅ = -b/w‚ÇÅ. La ecuaci√≥n de la l√≠nea no puede escribirse en forma pendiente-ordenada al origen en este caso."
    },
    "line_basic": {
        "title": "Recuerda la recta b√°sica",
        "description_p1": "Recordemos que en un plano cartesiano, podemos graficar una recta siguiendo la siguiente ecuaci√≥n:",
        "description_p2": "La variable **y** se representa en el eje vertical, mientras que la variable **x**, en el eje horizontal. La pendiente **m** determina la inclinaci√≥n de la recta, mientras que **b** es el punto donde la recta corta el eje **y**.",
        "exercise_desc":"Puedes jugar con los deslizadores para ver c√≥mo afectan a la recta el valor de la pendiente **m** y el corte **b**",
        "slope": "Pendiente (m)",
        "intercept": "Ordenada al origen (b)",
        "plot_title": "Ecuaci√≥n de la Recta: y = mx + b"
    },
    "linear_regression": {
        "title": "Regresi√≥n Lineal",
        "description_p1": "La regresi√≥n lineal es un algoritmo f√°cil de entender. Consideramos que la relaci√≥n entre dos variables puede verse como una l√≠nea recta.",
        "ex_title": "Ejemplo de un escenario real",
        "ex_desc_p1": "Imagina que queremos predecir la altura de una persona (joven) en funci√≥n de su edad. Podemos usar la regresi√≥n lineal para encontrar una recta que se ajuste a los datos.",
        "ex_desc_p2": "En este caso, la variable independiente **x** es la edad y la variable dependiente **y** es la altura.",
        "ex_interactive_desc": "Puedes jugar con los deslizadores para encontrar una recta que pueda modelar los datos.",
        "ex_plot_title": "Ejemplo de Regresi√≥n Lineal usando la altura de una persona",
        "ex_plot_x_label": "Edad (a√±os)",
        "ex_plot_y_label": "Altura (cm)",
        "ex_rmse": "Ra√≠z del Error Cuadr√°tico Medio (RMSE):",
        "ex_hint": "Pista: Intenta minimizar el valor del Error (RMSE)",
        "error_eq_title": "¬øC√≥mo medimos el error?",
        "error_eq_desc_p1": "Para saber si nuestro modelo es bueno, medimos la distancia entre los valores predichos y los valores reales.",
        "error_eq_desc_p2": "Y eso representa el Error Cuadr√°tico Medio (MSE):",
        "error_eq_params": "‚Ä¢ Donde **n** es el n√∫mero de puntos\n\n‚Ä¢ **y·µ¢** es el valor real de para el punto i\n\n‚Ä¢ **≈∑·µ¢** es el valor predicho para el punto i\n\n‚Ä¢ Y cuanto menor sea el MSE, mejor ser√° el ajuste.",
        "error_eq_desc_p3": "Nota que medimos la diferencia entre el valor real **y·µ¢** y el valor predicho **≈∑·µ¢** para cada punto **i**.",
        "error_eq_desc_p4": "Elevar la diferencia al cuadrado tiene dos consecuencias:",
        "error_eq_desc_p5": "1. Los errores grandes son m√°s importantes que los errores peque√±os\n\n2. La diferencia siempre es positiva, as√≠ que no importa si el modelo predice m√°s o menos que el valor real, el error ser√° positivo.",
        "error_eq_desc_p6": "A este valor se le puede aplicar la ra√≠z cuadrada para obtener el error en las mismas unidades que los valores reales.",
        "slope": "Pendiente (m)",
        "intercept": "Ordenada al origen (b)",
        "best_fit": "L√≠nea de Mejor Ajuste",
        "data_points": "Puntos de Datos",
        "error_lines": "L√≠neas de Error",
        "show_errors": "Mostrar l√≠neas de error",
        "plot_title": "L√≠nea de Regresi√≥n Lineal y Puntos de Datos",
        "explanation_title": "Entendiendo la Regresi√≥n Lineal",
        "explanation_text": "‚Ä¢ Los puntos azules representan nuestros datos\n\n‚Ä¢ La l√≠nea roja es nuestra l√≠nea de predicci√≥n (y = mx + b)\n\n‚Ä¢ Las l√≠neas grises (opcionales) muestran los errores entre las predicciones y los valores reales\n\n‚Ä¢ Un buen ajuste minimiza estos errores\n\n‚Ä¢ El valor MSE nos dice qu√© tan bueno es nuestro ajuste - menor es mejor",
        "exercise_title": "Ejercicio Pr√°ctico",
        "exercise_desc": "Intenta encontrar la l√≠nea que mejor se ajuste a estos puntos ajustando la pendiente y la ordenada al origen. Observa c√≥mo cambia el Error Cuadr√°tico Medio mientras ajustas la l√≠nea.",
        "exercise_plot_title": "Encuentra la L√≠nea de Mejor Ajuste"
    }
} 