{
    "app": {
        "title": "Machine Learning Gratuito",
        "welcome": "Bem-vindo! üéì",
        "description": "Esta aplica√ß√£o fornece visualiza√ß√µes interativas para compreender conceitos de machine learning. Use o menu lateral para navegar pelas se√ß√µes.",
        "available_sections": "Se√ß√µes dispon√≠veis:",
        "navigation": "Navega√ß√£o",
        "language": "Idioma"
    },
    "perceptron": {
        "title": "O Perceptron",
        "description": "O perceptron √© um conceito chave, pois as redes neurais modernas usam este elemento como bloco de constru√ß√£o.\n\n√â uma neur√¥nio capaz de realizar classifica√ß√£o bin√°ria baseada em regress√£o linear e uma fun√ß√£o de ativa√ß√£o.",
        "weight1": "Peso w1",
        "weight2": "Peso w2",
        "bias": "Bias (b)",
        "class1": "Classe 1 (y=1)",
        "class_neg1": "Classe -1 (y=-1)",
        "decision_boundary": "Fronteira de decis√£o",
        "graph_title": "Classifica√ß√£o do Perceptron e Fronteira de Decis√£o",
        "equations_title": "Equa√ß√µes Matem√°ticas",
        "perceptron_eq_title": "Regra de Decis√£o do Perceptron:",
        "perceptron_eq_desc": "O perceptron classifica um ponto (x‚ÇÅ, x‚ÇÇ) baseado nesta equa√ß√£o:",
        "comparison_title": "Rela√ß√£o com o Perceptron:",
        "comparison_text": "A fronteira de decis√£o do nosso perceptron √© uma reta! Vamos comparar as duas equa√ß√µes:\n\n‚Ä¢ Forma padr√£o: x‚ÇÇ = mx‚ÇÅ + b\n‚Ä¢ Nossa fronteira: x‚ÇÇ = -($w_1$/$w_2$)x‚ÇÅ - (b/$w_2$)\n\nPortanto:\n‚Ä¢ A inclina√ß√£o (m) √© -w‚ÇÅ/w‚ÇÇ\n‚Ä¢ A ordenada na origem (b) √© -b/w‚ÇÇ",
        "boundary_eq_title": "Equa√ß√£o da Fronteira de Decis√£o:",
        "boundary_eq_desc": "A fronteira de decis√£o √© onde a sa√≠da do perceptron √© igual a zero. Resolvendo para x‚ÇÇ:",
        "explanation_title": "Entendendo os Par√¢metros:",
        "explanation_text": "‚Ä¢ w‚ÇÅ, w‚ÇÇ: Pesos que determinam a inclina√ß√£o da fronteira de decis√£o\n\n‚Ä¢ b: Termo de bias, determina onde a linha intercepta o eixo x‚ÇÇ (deslocamento vertical)\n‚Ä¢ A fronteira de decis√£o √© a linha que separa as duas classes\n‚Ä¢ Pontos acima da linha s√£o classificados como uma classe, pontos abaixo como a outra\n‚Ä¢ Quando w‚ÇÇ = 0, a fronteira se torna uma linha vertical em x‚ÇÅ = -b/w‚ÇÅ",
        "exercise_title": "Exerc√≠cio Pr√°tico",
        "exercise_desc": "Tente classificar estes pontos ajustando os pesos e o bias. O objetivo √© encontrar uma linha que separe os pontos azuis dos vermelhos.",
        "exercise_plot_title": "Classifica√ß√£o Pr√°tica",
        "exercise_success": "Parab√©ns! Voc√™ classificou corretamente todos os pontos.",
        "exercise_continue": "Continue tentando! Ajuste os pesos e o bias para encontrar uma linha que separe as classes.",
        "limitations_title": "Limita√ß√µes do Perceptron",
        "limitations_desc": "O perceptron s√≥ pode aprender padr√µes linearmente separ√°veis. Aqui est√° um exemplo de um conjunto de dados n√£o linearmente separ√°vel que um perceptron n√£o pode classificar corretamente:",
        "limitations_plot_title": "Dados N√£o Linearmente Separ√°veis",
        "limitations_text": "Neste exemplo, temos duas classes de pontos que formam um padr√£o circular. Nenhuma linha reta (a fronteira de decis√£o do perceptron) pode separar estes pontos corretamente. Esta √© uma das principais limita√ß√µes do modelo perceptron.",
        "division_by_zero": "Quando w‚ÇÇ = 0, a fronteira de decis√£o se torna uma linha vertical em x‚ÇÅ = -b/w‚ÇÅ. A equa√ß√£o da linha n√£o pode ser escrita na forma inclina√ß√£o-ordenada na origem neste caso."
    },
    "line_basic": {
        "title": "Lembre-se da reta b√°sica",
        "description_p1": "Lembre-se que em um plano cartesiano, podemos graficar uma reta seguindo a seguinte equa√ß√£o:",
        "description_p2": "A vari√°vel **y** √© representada no eixo vertical, enquanto a vari√°vel **x** est√° no eixo horizontal. A inclina√ß√£o **m** determina a inclina√ß√£o da reta, enquanto **b** √© o ponto onde a reta corta o eixo **y**.",
        "exercise_desc": "Voc√™ pode brincar com os deslizadores para ver como a inclina√ß√£o **m** e o intercepto **b** afetam a reta",
        "slope": "Inclina√ß√£o (m)",
        "intercept": "Ordenada na origem (b)",
        "plot_title": "Equa√ß√£o da Reta: y = mx + b"
    },
    "linear_regression": {
        "title": "Regress√£o Linear",
        "description_p1": "A regress√£o linear √© um algoritmo f√°cil de entender. Consideramos que a rela√ß√£o entre duas vari√°veis pode ser vista como uma linha reta.",
        "ex_title": "Exemplo de um cen√°rio real",
        "ex_desc_p1": "Imagine que queremos prever a altura de uma pessoa (jovem) em fun√ß√£o da sua idade. Podemos usar regress√£o linear para encontrar uma reta que se ajuste aos dados.",
        "ex_desc_p2": "Neste caso, a vari√°vel independente **x** √© a idade e a vari√°vel dependente **y** √© a altura.",
        "ex_interactive_desc": "Voc√™ pode brincar com os deslizadores para encontrar uma reta que possa modelar os dados.",
        "ex_plot_title": "Exemplo de Regress√£o Linear usando a altura de uma pessoa",
        "ex_plot_x_label": "Idade (anos)",
        "ex_plot_y_label": "Altura (cm)",
        "ex_rmse": "Raiz do Erro Quadr√°tico M√©dio (RMSE):",
        "ex_hint": "Dica: Tente minimizar o valor do Erro (RMSE)",
        "error_eq_title": "Como medimos o erro?",
        "error_eq_desc_p1": "Para saber se nosso modelo √© bom, medimos a dist√¢ncia entre os valores previstos e os valores reais.",
        "error_eq_desc_p2": "E isso representa o Erro Quadr√°tico M√©dio (MSE):",
        "error_eq_params": "‚Ä¢ Onde **n** √© o n√∫mero de pontos\n\n‚Ä¢ **y·µ¢** √© o valor real para o ponto i\n\n‚Ä¢ **≈∑·µ¢** √© o valor previsto para o ponto i\n\n‚Ä¢ E quanto menor o MSE, melhor o ajuste.",
        "error_eq_desc_p3": "Note que medimos a diferen√ßa entre o valor real **y·µ¢** e o valor previsto **≈∑·µ¢** para cada ponto **i**.",
        "error_eq_desc_p4": "Elevar a diferen√ßa ao quadrado tem duas consequ√™ncias:",
        "error_eq_desc_p5": "1. Erros grandes s√£o mais importantes que erros pequenos\n\n2. A diferen√ßa √© sempre positiva, ent√£o n√£o importa se o modelo prev√™ mais ou menos que o valor real, o erro ser√° positivo.",
        "error_eq_desc_p6": "Este valor pode ser aplicada a raiz quadrada para obter o erro nas mesmas unidades que os valores reais.",
        "slope": "Inclina√ß√£o (m)",
        "intercept": "Ordenada na origem (b)",
        "best_fit": "Linha de Melhor Ajuste",
        "data_points": "Pontos de Dados",
        "error_lines": "Linhas de Erro",
        "show_errors": "Mostrar linhas de erro",
        "plot_title": "Linha de Regress√£o Linear e Pontos de Dados",
        "explanation_title": "Entendendo a Regress√£o Linear",
        "explanation_text": "‚Ä¢ Assumimos que existe uma rela√ß√£o linear entre duas vari√°veis.\n\n‚Ä¢ A maneira mais f√°cil de perceber uma rela√ß√£o linear √© graficar os valores em um plano cartesiano.\n\n‚Ä¢ Nenhum modelo √© perfeito, sempre existir√° algum erro porque o mundo real n√£o tem linhas retas.\n\n‚Ä¢ Se voc√™ quiser usar este modelo adequadamente, recomendo aprofundar estes conceitos: distribui√ß√£o normal, correla√ß√£o e m√≠nimos quadrados ordin√°rios (para os amantes da matem√°tica).\n\n‚Ä¢ Finalmente, se voc√™ quiser implementar este modelo, √© t√£o f√°cil quanto usar uma biblioteca Python, o dif√≠cil √© entender a teoria.",
        "limitations_title": "Limita√ß√µes",
        "limitations_text": "Existem outras rela√ß√µes no mundo real que n√£o seguem uma linha reta, por exemplo:"
    }
} 