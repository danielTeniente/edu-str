{
    "app": {
        "title": "Free Machine Learning",
        "welcome": "Welcome! üéì",
        "description": "This application provides interactive visualizations to understand machine learning concepts. Use the sidebar menu to navigate through sections.",
        "available_sections": "Available sections:",
        "navigation": "Navigation",
        "language": "Language"
    },
    "perceptron": {
        "title": "The Perceptron",
        "description": "The perceptron is a key concept, as modern neural networks use this element as a building block.\n\nIt is a neuron capable of performing binary classification based on linear regression and an activation function.",
        "weight1": "Weight w1",
        "weight2": "Weight w2",
        "bias": "Bias (b)",
        "class1": "Class 1 (y=1)",
        "class_neg1": "Class -1 (y=-1)",
        "decision_boundary": "Decision boundary",
        "graph_title": "Perceptron Classification and Decision Boundary",
        "equations_title": "Mathematical Equations",
        "perceptron_eq_title": "Perceptron Decision Rule:",
        "perceptron_eq_desc": "The perceptron classifies a point (x‚ÇÅ, x‚ÇÇ) based on this equation:",
        "comparison_title": "Relationship with the Perceptron:",
        "comparison_text": "Our perceptron's decision boundary is a line! Let's compare both equations:\n\n‚Ä¢ Standard form: x‚ÇÇ = mx‚ÇÅ + b\n‚Ä¢ Our boundary: x‚ÇÇ = -($w_1$/$w_2$)x‚ÇÅ - (b/$w_2$)\n\nTherefore:\n‚Ä¢ The slope (m) is -w‚ÇÅ/w‚ÇÇ\n‚Ä¢ The y-intercept (b) is -b/w‚ÇÇ",
        "boundary_eq_title": "Decision Boundary Equation:",
        "boundary_eq_desc": "The decision boundary is where the perceptron's output equals zero. Solving for x‚ÇÇ:",
        "explanation_title": "Understanding the Parameters:",
        "explanation_text": "‚Ä¢ w‚ÇÅ, w‚ÇÇ: Weights that determine the slope of the decision boundary\n\n‚Ä¢ b: Bias term, determines where the line intersects the x‚ÇÇ axis (vertical shift)\n‚Ä¢ The decision boundary is the line that separates the two classes\n‚Ä¢ Points above the line are classified as one class, points below as the other\n‚Ä¢ When w‚ÇÇ = 0, the boundary becomes a vertical line at x‚ÇÅ = -b/w‚ÇÅ",
        "exercise_title": "Practice Exercise",
        "exercise_desc": "Try to classify these points by adjusting the weights and bias. The goal is to find a line that separates the blue points from the red ones.",
        "exercise_plot_title": "Practical Classification",
        "exercise_success": "Congratulations! You have correctly classified all points.",
        "exercise_continue": "Keep trying! Adjust the weights and bias to find a line that separates the classes.",
        "limitations_title": "Perceptron Limitations",
        "limitations_desc": "The perceptron can only learn linearly separable patterns. Here's an example of a non-linearly separable dataset that a perceptron cannot classify correctly:",
        "limitations_plot_title": "Non-linearly Separable Data",
        "limitations_text": "In this example, we have two classes of points that form a circular pattern. No straight line (the perceptron's decision boundary) can separate these points correctly. This is one of the main limitations of the perceptron model.",
        "division_by_zero": "When w‚ÇÇ = 0, the decision boundary becomes a vertical line at x‚ÇÅ = -b/w‚ÇÅ. The line equation cannot be written in slope-intercept form in this case."
    },
    "line_basic": {
        "title": "Remember the basic line",
        "description_p1": "Recall that in a Cartesian plane, we can graph a line following this equation:",
        "description_p2": "The variable **y** is represented on the vertical axis, while the variable **x** is on the horizontal axis. The slope **m** determines the line's inclination, while **b** is the point where the line cuts the **y** axis.",
        "exercise_desc": "You can play with the sliders to see how the slope **m** and intercept **b** affect the line",
        "slope": "Slope (m)",
        "intercept": "Y-intercept (b)",
        "plot_title": "Line Equation: y = mx + b"
    },
    "linear_regression": {
        "title": "Linear Regression",
        "description_p1": "Linear regression is an easy-to-understand algorithm. We consider that the relationship between two variables can be seen as a straight line.",
        "ex_title": "Example of a real scenario",
        "ex_desc_p1": "Imagine we want to predict a person's height (young) based on their age. We can use linear regression to find a line that fits the data.",
        "ex_desc_p2": "In this case, the independent variable **x** is age and the dependent variable **y** is height.",
        "ex_interactive_desc": "You can play with the sliders to find a line that can model the data.",
        "ex_plot_title": "Linear Regression Example using person's height",
        "ex_plot_x_label": "Age (years)",
        "ex_plot_y_label": "Height (cm)",
        "ex_rmse": "Root Mean Squared Error (RMSE):",
        "ex_hint": "Hint: Try to minimize the Error value (RMSE)",
        "error_eq_title": "How do we measure error?",
        "error_eq_desc_p1": "To know if our model is good, we measure the distance between predicted values and real values.",
        "error_eq_desc_p2": "And that represents the Mean Squared Error (MSE):",
        "error_eq_params": "‚Ä¢ Where **n** is the number of points\n\n‚Ä¢ **y·µ¢** is the real value for point i\n\n‚Ä¢ **≈∑·µ¢** is the predicted value for point i\n\n‚Ä¢ And the lower the MSE, the better the fit.",
        "error_eq_desc_p3": "Note that we measure the difference between the real value **y·µ¢** and the predicted value **≈∑·µ¢** for each point **i**.",
        "error_eq_desc_p4": "Squaring the difference has two consequences:",
        "error_eq_desc_p5": "1. Large errors are more important than small errors\n\n2. The difference is always positive, so it doesn't matter if the model predicts more or less than the real value, the error will be positive.",
        "error_eq_desc_p6": "This value can be square rooted to get the error in the same units as the real values.",
        "slope": "Slope (m)",
        "intercept": "Y-intercept (b)",
        "best_fit": "Best Fit Line",
        "data_points": "Data Points",
        "error_lines": "Error Lines",
        "show_errors": "Show error lines",
        "plot_title": "Linear Regression Line and Data Points",
        "explanation_title": "Understanding Linear Regression",
        "explanation_text": "‚Ä¢ We assume there is a linear relationship between two variables.\n\n‚Ä¢ The easiest way to perceive a linear relationship is to graph the values on a Cartesian plane.\n\n‚Ä¢ No model is perfect, there will always be some error because the real world doesn't have straight lines.\n\n‚Ä¢ If you want to use this model properly, I recommend delving into these concepts: normal distribution, correlation, and ordinary least squares (for math lovers).\n\n‚Ä¢ Finally, if you want to implement this model, it's as easy as using a Python library, the hard part is understanding the theory.",
        "limitations_title": "Limitations",
        "limitations_text": "There are other relationships in the real world that don't follow a straight line, for example:"
    },
    "books": {
        "title": "Support the Project",
        "description": "If you find this educational platform helpful, you can support its development by purchasing one of my technical books:",
        "neural_networks": {
            "title": "Trapped in Neural Networks",
            "description": "A book where I share my experience working in the world of artificial intelligence. I talk a bit about technology, but above all, I want to tell a story that guides new students who want to enter this world.",
            "price": "$5.99"
        },
        "competitive_programming": {
            "title": "A Very Complex World",
            "description": "A popular science book about competitive programming. I'll teach you the fundamentals to reason like the world's best programmers.",
            "price": "$6.99"
        },
        "thank_you": "Thank you for your support!",
        "note": "Note: These books are available in Spanish."
    }
} 